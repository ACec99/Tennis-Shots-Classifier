{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L23NlcPhHVke"
      },
      "source": [
        "# **SETTINGS AND IMPORTS**\n",
        "Set the hyperparameters, import all the needed libraries and mount Google Drive to retrieve data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lehSH8fvxnFc"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 300\n",
        "NUM_HIDDEN = 128\n",
        "NUM_FEATURES = 36 #(2 X 18)\n",
        "NUM_CLASSES = 10\n",
        "NUM_LAYERS = 3\n",
        "BATCH_SIZE = 32\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "APPLY_WEIGHTS = True\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT_2D = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyKdNHRcflA9"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmbH9gIhrXVN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "import traceback\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms \n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import MulticlassF1Score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import seaborn as sns\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0yjSXs803qf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub8ynPlaxdIa"
      },
      "source": [
        "# **OPENPOSE INSTALLATION**\n",
        "**EXECUTE ONLY FOR THE FIRST TIME!** \n",
        "\n",
        "Install the pre-trained model called [*OpenPose*](https://github.com/CMU-Perceptual-Computing-Lab/openpose.git), which lets us extract the body_joints from the videos (pose in COCO standard).\n",
        "\n",
        "Note that: to use COCO standard, we modified the \"openpose/CMakeLists.txt\" file in order to build an instance of OpenPose based only on COCO model (by default the installation process downloads only BODY_25 model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHRPw45pB5EK"
      },
      "outputs": [],
      "source": [
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  #!pip install -q youtube-dl\n",
        "  # build openpose\n",
        "\n",
        "  !cd openpose\n",
        "\n",
        "  torch.backends.cudnn.enabled = True\n",
        "\n",
        "  #after that we have to modify the CMake file ( put ON the download of COCO and OFF the others )\n",
        "\n",
        "  with open(\"openpose/CMakeLists.txt\", \"a\") as file:\n",
        "    file.write(\"option(USE_CUDNN \\\"Use CUDNN library\\\" ON)\\n\")\n",
        "\n",
        "  result1 = subprocess.run([\"sed\", \"-i\", \"s/option(DOWNLOAD_BODY_COCO_MODEL \\\"Download body 18-keypoint COCO model.\\\" OFF)/option(DOWNLOAD_BODY_COCO_MODEL \\\"Download body 18-keypoint COCO model.\\\" ON)/g\", \"/content/openpose/CMakeLists.txt\"], capture_output=True, text=True)\n",
        "  result2 = subprocess.run([\"sed\", \"-i\", \"s/option(DOWNLOAD_BODY_25_MODEL \\\"Download body 25-keypoint (body COCO and 6-keypoint foot) model.\\\" ON)/option(DOWNLOAD_BODY_25_MODEL \\\"Download body 25-keypoint (body COCO and 6-keypoint foot) model.\\\" OFF)/g\", \"/content/openpose/CMakeLists.txt\"], capture_output=True, text=True)\n",
        "  result3 = subprocess.run([\"sed\", \"-i\", \"s/option(DOWNLOAD_FACE_MODEL \\\"Download face model.\\\" ON)/option(DOWNLOAD_FACE_MODEL \\\"Download face model.\\\" OFF)/g\", \"/content/openpose/CMakeLists.txt\"], capture_output=True, text=True)\n",
        "  result4 = subprocess.run([\"sed\", \"-i\", \"s/option(DOWNLOAD_HAND_MODEL \\\"Download hand model.\\\" ON)/option(DOWNLOAD_HAND_MODEL \\\"Download hand model.\\\" OFF)/g\", \"/content/openpose/CMakeLists.txt\"], capture_output=True, text=True)\n",
        "\n",
        "  if result1.returncode != 0:\n",
        "    print(\"Failed to run the command\")\n",
        "    print(\"Error message:\", result1.stderr)\n",
        "  elif result2.returncode != 0:\n",
        "    print(\"Failed to run the command\")\n",
        "    print(\"Error message:\", result2.stderr)\n",
        "  elif result3.returncode != 0:\n",
        "    print(\"Failed to run the command\")\n",
        "    print(\"Error message:\", result3.stderr)\n",
        "  elif result4.returncode != 0:\n",
        "    print(\"Failed to run the command\")\n",
        "    print(\"Error message:\", result4.stderr)\n",
        "  else:\n",
        "    print(result1.stdout)\n",
        "    print(result2.stdout)\n",
        "    print(result3.stdout)\n",
        "    print(result4.stdout)\n",
        "\n",
        "    !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElaxDNXe5NRs"
      },
      "source": [
        "# **CREATION OF THE DATASET**\n",
        "**EXECUTE ONLY FOR THE FIRST TIME!** \n",
        "\n",
        "1. Take the [*THETIS Dataset*](http://thetis.image.ece.ntua.gr/) (we chose to use only RGB videos, to see all the other folders click the link).\n",
        "2. Extract body_joints (x, y, confidence_score) from each frame of every video in the dataset\n",
        "3. Create a dataset list with each video represented by a label (that indicates the type of shot) and a list of frames (with padding, where needed)\n",
        "4. Convert it into a csv file.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpok8y-ydbC7"
      },
      "outputs": [],
      "source": [
        "#unzip the THETIS DATASET (we chose to use only RGB videos)\n",
        "!unzip /content/drive/MyDrive/Vision_Project_Utils/VIDEO_RGB.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f2D0Apy4guy"
      },
      "outputs": [],
      "source": [
        "#build the initial video dataset table, by extracting video paths from the VIDEO_RGB folder\n",
        "\n",
        "dir_dataset = os.listdir('/content/drive/MyDrive/Vision_Project_Utils/VIDEO_RGB')\n",
        "\n",
        "shots = []\n",
        "\n",
        "for channel in dir_dataset:\n",
        "  dir_specific_shot = os.listdir( '/content/drive/MyDrive/Vision_Project_Utils/VIDEO_RGB' + '/' + channel)\n",
        "  for video_file in dir_specific_shot:\n",
        "    shots.append(((channel, str('/content/drive/MyDrive/Vision_Project_Utils/VIDEO_RGB' + '/' + channel) + video_file, str(video_file))))\n",
        "\n",
        "temp_dataset = pd.DataFrame(data=shots,columns=['label','video_path','video_name'])\n",
        "print(temp_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRiheXK3B4uE"
      },
      "outputs": [],
      "source": [
        "#method to extract body_joints from one video\n",
        "\n",
        "def extract_features(label_name,complete_name):\n",
        "  video_name = complete_name.split(\".\")[0]\n",
        "  dir_frames_video = '/content/frames/' + video_name + '_tmp_frames'\n",
        "  dir_output_json = '/content/videos_joints/' + video_name + '_output_json/'\n",
        "  try:\n",
        "    os.makedirs(dir_frames_video)\n",
        "  except OSError as e1:\n",
        "    print(\"Creation of the directory %s failed\" % dir_frames_video)\n",
        "    print(\"Error:\", e1)\n",
        "    print(traceback.format_exc())\n",
        "  try:\n",
        "    os.makedirs(dir_output_json)\n",
        "  except OSError as e2: \n",
        "    print(\"Creation of the directory %s failed\" % dir_output_json)\n",
        "    print(\"Error:\", e2)\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "  input_file = \"/content/drive/MyDrive/Vision_Project_Utils/VIDEO_RGB\" + label_name + \"/\" + complete_name\n",
        "  output_file = '/content/frames/' + video_name + '_tmp_frames/%09d.png'\n",
        "\n",
        "  #divide videos in frames\n",
        "  try:\n",
        "    subprocess.run([\"ffmpeg\", \"-i\", input_file, output_file], check=True)\n",
        "  except subprocess.CalledProcessError as e:\n",
        "    print(\"ffmpeg command failed:\", e)\n",
        "\n",
        "  # detect poses \n",
        "  !rm openpose.avi\n",
        "\n",
        "  image_dir = '/content/frames/' + video_name + '_tmp_frames/'\n",
        "  output_json = dir_output_json\n",
        "\n",
        "  #takes frames folder and, for each frame, extracts body_joint features and put them in output_json\n",
        "  command = f\"cd openpose && ./build/examples/openpose/openpose.bin --model_pose COCO --image_dir {image_dir} --write_json {output_json} --display 0 --number_people_max 1 --write_video_fps 18 --write_video ../openpose.avi\"\n",
        "\n",
        "  \n",
        "  process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  stdout, stderr = process.communicate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xqMGlPqfmq1"
      },
      "source": [
        "Extraction of the features (joints) for each frame of each video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlXzUcrScf3Q"
      },
      "outputs": [],
      "source": [
        "dir_videos_joints = \"/content/videos_joints/\"\n",
        "dir_frames = \"/content/frames/\"\n",
        "\n",
        "try:\n",
        "  os.makedirs(dir_frames)\n",
        "except OSError as e1:\n",
        "  print(\"Creation of the directory %s failed\" % dir_frames)\n",
        "  print(\"Error:\", e1)\n",
        "  print(traceback.format_exc())\n",
        "\n",
        "try:\n",
        "  os.makedirs(dir_videos_joints)\n",
        "except OSError as e2:\n",
        "  print(\"Creation of the directory %s failed\" % dir_videos_joints)\n",
        "  print(\"Error:\", e2)\n",
        "  print(traceback.format_exc())\n",
        "\n",
        "#take from temp_dataset (a pd.DataFrame) the useful data to build the dataset, and put them in videos_joints folder\n",
        "for index, row in temp_dataset.iterrows():\n",
        "  extract_features(row['label'],row['video_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o6588nlr0eG"
      },
      "outputs": [],
      "source": [
        "#takes as input one json file, representing one frame, and converts it into an array of joint coordinates\n",
        "\n",
        "def json_to_list(dir_json_files, file_json):\n",
        "\n",
        "  try:\n",
        "    my_json_file = open(os.path.join(dir_json_files,file_json),'r') #open the particular json file corresponding to the currently analized frame\n",
        "    json_data = my_json_file.read()\n",
        "    obj = json.loads(json_data) #convert json file to a python object\n",
        "    array_joints = obj['people']# get(\"pose_keypoints_2d\") \n",
        "\n",
        "    if not array_joints:\n",
        "      print(\"my list of people is empty\")\n",
        "      return []\n",
        "    else:\n",
        "      array_joints = obj['people'][0][\"pose_keypoints_2d\"]\n",
        "      return array_joints\n",
        "  except FileNotFoundError:\n",
        "    print(\"my_json_file not found.\")\n",
        "  except:\n",
        "    print(\"An error occurred while loading the my_json_file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI_jVG5Xr1Sb"
      },
      "outputs": [],
      "source": [
        "#initialize the labels related to the different tennis strokes\n",
        "\n",
        "dict_labels = {\n",
        "    'backhand': 0,\n",
        "    #'bslice': 1, since there aren't enough videos to analize, we decided to dump them \n",
        "    #'bvolley': 2,\n",
        "    'backhand2h': 1,\n",
        "    'serflat': 2,\n",
        "    'foreflat': 3,\n",
        "    'foreopen': 4,\n",
        "    'fslice': 5,\n",
        "    'fvolley': 6,\n",
        "    'serkick': 7,\n",
        "    'serslice': 8,\n",
        "    'smash': 9\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1YRRc_S-vle"
      },
      "outputs": [],
      "source": [
        "#create the dataset list, strarting from videos_joints folder\n",
        "\n",
        "def create_dataset(dir_dataset, dict_labels): \n",
        "  dataset = []\n",
        "  i=0\n",
        "  for folder in dir_dataset:\n",
        "    dir_folder = \"/content/drive/MyDrive/Vision_Project_Utils/videos_joints/\" + folder\n",
        "    dir_folder_list = os.listdir(dir_folder)\n",
        "    video = []\n",
        "    name = str(folder)\n",
        "    label_name = name.split('_')[1]\n",
        "    if label_name in dict_labels:\n",
        "      class_idx = dict_labels[label_name]\n",
        "      for f_json in dir_folder_list:\n",
        "        frame = json_to_list(dir_folder,f_json) \n",
        "        video.append(frame)\n",
        "      video.insert(0,class_idx)\n",
        "      dataset.append(video)\n",
        "      print(\"Inseriti \"+str(i)+\" video nel dataset!\")\n",
        "      i+=1\n",
        "  return dataset\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2jMg_iiqOB4"
      },
      "outputs": [],
      "source": [
        "#create and save the list corresponding to our dataset in a \".pickle\" file (binary file)\n",
        "\n",
        "dir_dataset = os.listdir(\"/content/drive/MyDrive/Vision_Project_Utils/videos_joints\")\n",
        "dataset= create_dataset(dir_dataset, dict_labels)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Vision_Project_Utils/dataset.pickle\", \"wb\") as file:\n",
        "    pickle.dump(dataset, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVUV-YmrinMl"
      },
      "outputs": [],
      "source": [
        "#method used to compute the padding frame for a specific video\n",
        "#Since a body_joint consists in (x,y,confidence_score) coordinates,\n",
        "#we compute the average of x and y among all the frames,\n",
        "#while we put as third coordinate (corresponding to the confidence_score) a low value (since it's not a true body_joint/frame)\n",
        "\n",
        "def mean_frame_vector(video):\n",
        "    mean_frame = []\n",
        "    for i in range(54): #frame dimension\n",
        "        if (i+1)%3 != 0:\n",
        "          flattened = [sublist[i] for sublist in video] \n",
        "          mean = sum(flattened) / len(flattened)\n",
        "          mean_frame.append(mean)\n",
        "        else: #it's a confidence score\n",
        "          mean_frame.append(0.01) #low confidence score assigned to the mean frame vector\n",
        "    return mean_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjlVt6x4ikA3"
      },
      "outputs": [],
      "source": [
        "#creates the dataset pd.DataFrame from the dataset list, \n",
        "#adding the padding frames to each video that has less frames than the max number of frames \n",
        "\n",
        "def create_dataset_PD(dataset):\n",
        "\n",
        "  max_cols = max([len(video) - 1 for video in dataset]) #from the longest video, we obtain the max number of frames \n",
        "\n",
        "  df = pd.DataFrame(columns=['label'] + [f'frame_{i}' for i in range(max_cols)])\n",
        "\n",
        "  for video in dataset:\n",
        "    if video and isinstance(video[0], int):\n",
        "      label = video[0]\n",
        "      frames = video[1:]\n",
        "      if frames:\n",
        "        frames = frames + [mean_frame_vector(frames) for j in range(max_cols - len(frames))]\n",
        "        df.loc[i] = ([label] + frames)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6HdoAbfPAiu"
      },
      "outputs": [],
      "source": [
        "#takes the dataset list from the binary pickle file\n",
        "with open(\"/content/drive/MyDrive/Vision_Project_Utils/dataset.pickle\", \"rb\") as file:\n",
        "    dataset_loaded = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVNhrtUTNEmG"
      },
      "outputs": [],
      "source": [
        "#delete empty frames in videos\n",
        "for i in range(len(dataset_loaded)):\n",
        "  dataset_loaded[i][1:] = list(filter(None, dataset_loaded[i][1:]))\n",
        "#creation of the pandas table of the dataset (composed by the label and the original joint values for each frame of each video)\n",
        "table = create_dataset_PD(dataset_loaded)\n",
        "print(table)\n",
        "#creation of the dataset csv\n",
        "table.to_csv('/content/drive/MyDrive/Vision_Project_Utils/dataset.csv',index=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djLkRQM_VZrl"
      },
      "source": [
        "# **DATASET SUDDIVISION & PREPARATION**\n",
        "Preprocessing of the dataset, with:\n",
        "*   Affine transformation, to make data invariant to body position and dimension\n",
        "*   Split in training, validation and testing sets\n",
        "*   Normalization\n",
        "*   Creation of Dataloaders\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlnhuFG05hyz"
      },
      "source": [
        "**First suddivision**\n",
        "- training set\n",
        "- testing set\n",
        "\n",
        "**Second suddivision**: we have to split another time the training set, in order to form the validation set\n",
        "- training set\n",
        "- validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FmeoqkTviv5K"
      },
      "outputs": [],
      "source": [
        "#from the dataset.csv file, we extract the data and labels lists (in numpy format)\n",
        "\n",
        "data_csv = pd.read_csv('/content/drive/MyDrive/Vision_Project_Utils/dataset.csv',delimiter=',',header=None)\n",
        "\n",
        "videos_list = []\n",
        "labels_list = []\n",
        "for i in range(1, len(data_csv)):\n",
        "  frames_list = []\n",
        "  for j in range(len((data_csv).columns)):\n",
        "    if j == 0:\n",
        "      labels_list.append(int(data_csv.iloc[i,j]))\n",
        "    else:\n",
        "      frames_list.append(np.array((eval(data_csv.iloc[i,j])),dtype=float))\n",
        "  frames_list = np.array(frames_list)\n",
        "  videos_list.append(frames_list)\n",
        "\n",
        "data = np.array(videos_list)\n",
        "labels = np.array(labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AgTCb8NZooan"
      },
      "outputs": [],
      "source": [
        "#compute the affine transform matrix, \n",
        "#with scale and translation terms based on the mean and standard deviation of shoulders body_joint coordinates\n",
        "\n",
        "total_shoulders = []\n",
        "\n",
        "for video in data:\n",
        "  shoulders = np.array([np.concatenate((frame[6:8],frame[15:17])) for frame in video])\n",
        "  mean_shoulders = np.mean(shoulders, axis=0)\n",
        "  total_shoulders.append(mean_shoulders)\n",
        "\n",
        "total_shoulders = np.array(total_shoulders)\n",
        "mean__total_shoulders = np.mean(total_shoulders, axis=0)\n",
        "total_shoulders_centered = total_shoulders - mean__total_shoulders\n",
        "std_total_shoulders = np.std(total_shoulders_centered)\n",
        "total_shoulders_normalized = total_shoulders_centered / std_total_shoulders\n",
        "\n",
        "s_x, s_y = np.std(total_shoulders_normalized, axis=0)[:2]\n",
        "t_x, t_y = -mean_shoulders[:2] / std_total_shoulders\n",
        "affine_matrix = np.array([[s_x, 0, t_x], [0, s_y, t_y], [0, 0, 1]])\n",
        "data_reshaped = data.reshape(1620,150,18,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXb4dTapt97P"
      },
      "outputs": [],
      "source": [
        "#apply the affine matrix to scale and translate all the other joints in each video:\n",
        "#in this way we obtain coordinates invariant w.r.t. body position and dimension! \n",
        "\n",
        "data_norm = []\n",
        "for i in range(len(data_reshaped)):\n",
        "  video = data_reshaped[i]\n",
        "  normalized_joints = []\n",
        "  for frame in video:\n",
        "    base = np.zeros_like(frame)\n",
        "    coord = frame[:,:2]\n",
        "    weights = frame[:,2]\n",
        "\n",
        "    joints_homogeneous = np.concatenate([coord, np.ones((coord.shape[0], 1))], axis=1)\n",
        "    normalized_joints_homogeneous = np.dot(joints_homogeneous, affine_matrix)\n",
        "    base[:,:2] = normalized_joints_homogeneous[:, :2]\n",
        "    base[:,2] = weights\n",
        "    normalized_joints.append(base)\n",
        "\n",
        "  normalized_joints = np.array(normalized_joints)\n",
        "  data_norm.append(normalized_joints)\n",
        "\n",
        "data_norm = np.array(data_norm).reshape(1620,150,54)\n",
        "\n",
        "print(data_norm.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiZw3Ww9NQms"
      },
      "source": [
        "Suddivision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8QRaVxO_oHeP"
      },
      "outputs": [],
      "source": [
        "#suddivision in training set and testing sets (proportion of target classes is maintained in both sets by using \"stratify\" parameter)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data_norm, labels, test_size=0.2, shuffle=True, stratify=labels) \n",
        "\n",
        "#second suddivision in order to create validation set => suddivision of the training set into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.4, stratify=train_labels) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E7-ikWUahKPV"
      },
      "outputs": [],
      "source": [
        "# Define a custom normalization function that normalizes only x and y values\n",
        "\n",
        "def normalize(frame, mean, std):\n",
        "  normalized_frame = np.zeros_like(frame)\n",
        "  normalized_frame[:, :2] = (frame[:, :2] - mean) / std\n",
        "  normalized_frame[:, 2] = frame[:, 2]\n",
        "  return normalized_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjh-4XbTm6wJ"
      },
      "source": [
        "Creation of the custom dataset class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A5DpmID-PR1R"
      },
      "outputs": [],
      "source": [
        "class BodyJointsDataset(Dataset): \n",
        "\n",
        "  def __init__(self, data, labels, mean, std, n_c): #remember! the mean and the std_deviation must be computed through all the training data.\n",
        "                                                        #Then, we will use them (mean and std_deviation) to normalize the training, validation and testing data\n",
        "\n",
        "    self.X = data\n",
        "\n",
        "    self.Y = labels\n",
        "    \n",
        "    self.mean = mean\n",
        "\n",
        "    self.std = std\n",
        "\n",
        "    self.num_classes = n_c\n",
        "\n",
        "  def __len__(self): #it must return the total number of samples that we have in the dataset\n",
        "\n",
        "        return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx_sample): #getitem must return the set of frames of a specific video \"X\" (in position idx_sample) and its specific label \"y\"\n",
        "                                    # Before returning, X is normalized and transformed into a tensor\n",
        "    X , y = self.X[idx_sample] , self.Y[idx_sample]\n",
        "\n",
        "    # Apply the custom normalization function to the frames\n",
        "    X = np.array([normalize(frame, self.mean, self.std) for frame in X])\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    return X , y "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxObd03BNEi0"
      },
      "source": [
        "Preparation of sets and dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D8vhA3Nv5gvr"
      },
      "outputs": [],
      "source": [
        "#First, we need to reshape train_data (in order to compute mean and std only on (x,y), and not on confidence_score)\n",
        "new_train_data= train_data.reshape(train_data.shape[0], train_data.shape[1], 18, 3)\n",
        "#We also reshape the test and validation set\n",
        "new_test_data= test_data.reshape(test_data.shape[0], test_data.shape[1], 18, 3)\n",
        "new_val_data= val_data.reshape(val_data.shape[0], val_data.shape[1], 18, 3)\n",
        "\n",
        "# Extract x, y, and confidence score values from the frames in train_data\n",
        "train_data_xy= (new_train_data[:, :, :, :2])\n",
        "\n",
        "# Calculate the mean and std values for each (x, y) coordinate in train_data, in order to do Z-score normalization\n",
        "mean = train_data_xy.mean(axis=(0, 1, 2))\n",
        "std = train_data_xy.std(axis=(0, 1, 2))\n",
        "\n",
        "#initialize train, test and validation sets\n",
        "train_dataset = BodyJointsDataset(new_train_data, train_labels, mean, std, NUM_CLASSES)\n",
        "test_dataset = BodyJointsDataset(new_test_data, test_labels, mean, std, NUM_CLASSES)\n",
        "validation_dataset = BodyJointsDataset(new_val_data, val_labels, mean, std, NUM_CLASSES)\n",
        "\n",
        "# Create dataloaders for the training, testing, and validation sets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrC7Ce2byMRW"
      },
      "source": [
        "# **MODEL IMPLEMENTATION & TRAINING PHASE**\n",
        "1. LSTM-Based Model with:\n",
        "  *   Dropout2d Layer\n",
        "  *   Bidirectional LSTM (with layers=num_layers)\n",
        "  *   Attention Layer (with Dot-Product Attention)\n",
        "  *   Output Layer (Fully-Connected layer)\n",
        "  \n",
        "  Note that Dropout2d Layer, Bidirectional and Attention Layer can be enabled/disabled through hyperparameters.\n",
        "2. TRAINER, with both Training and Validation Phases\n",
        "3. EXECUTE, with:\n",
        "  *   model = TennisShotsClassificationLSTM object\n",
        "  *   loss = CrossEntropyLoss \n",
        "  *   optimizer = Adagrad\n",
        "4. PLOT Loss, Accuracy and F1-Score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-TKGmvcbr-uY"
      },
      "outputs": [],
      "source": [
        "class TennisShotsClassificationLSTM(nn.Module):\n",
        "  def __init__(self, h_s:int, n_f:int, n_c:int, n_l:int, b_s:int, d_o:float, apply_weights:bool, bd:bool):\n",
        "    super(TennisShotsClassificationLSTM,self).__init__()\n",
        "    \n",
        "    #if we choose a Bidirectional-LSTM, we double the hidden_size\n",
        "    if bd == True:\n",
        "      self.hidden_size = 2*h_s\n",
        "    else:\n",
        "      self.hidden_size = h_s\n",
        "\n",
        "    self.num_features = n_f\n",
        "    self.num_classes = n_c\n",
        "    self.num_layers = n_l\n",
        "    self.batch_size = b_s\n",
        "    self.dropout = d_o\n",
        "    self.apply_weights = apply_weights\n",
        "    self.bidirectional = bd\n",
        "\n",
        "    #This dropout acts on the singular joints: in this way we deactivate some joints (with a probability equal to DROPOUT_2D), \n",
        "    #before passing the input to the LSTM\n",
        "    self.dropout = nn.Dropout2d(p=DROPOUT_2D)\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=n_f, \n",
        "                        hidden_size=h_s, \n",
        "                        num_layers=n_l, \n",
        "                        batch_first=True,\n",
        "                        dropout = d_o,\n",
        "                        bidirectional=bd\n",
        "                        )\n",
        "\n",
        "    #ATTENTION LAYER\n",
        "    self.query = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "    self.key = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "    self.value = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    self.softmax = nn.LogSoftmax(dim=2)                    \n",
        "    self.classifier = nn.Linear(self.hidden_size,n_c)\n",
        "\n",
        "  def forward(self,sequence: torch.Tensor): #sequence represents the tensor of dimension (batch_size, num_frames, num_features)\n",
        "    #To make predictions not influenced by padding frames, we apply a mask to each video (1 for true frames, 0 for padding frames)\n",
        "    mask = torch.ones_like(sequence)\n",
        "    mask[sequence[:,:,:,2] == 0.01] = 0\n",
        "\n",
        "    seq_lengths = torch.tensor([]) #list(int), in which each int \"i\" represents the number of true frames in the i-th video\n",
        "    for video in mask:\n",
        "      tensor = torch.sum(video,dim=0)\n",
        "      seq_lengths = torch.cat((seq_lengths,torch.tensor([tensor[0][0]])),dim=0)\n",
        "\n",
        "    joints_sequence = sequence[:, :, :, :2]  # Extract joint data (shape: (batch_size, num_frames, num_joints, 2))\n",
        "    \n",
        "    if DROPOUT_2D != 0: #since Dropout2d acts only on the last dimension, we first have to permute the last two dimensions, \n",
        "                        #in order to execute dropout only on joints (and not on single coordinates, which would otherwise be changed in values)\n",
        "      joints_sequence = joints_sequence.permute(0, 1, 3, 2)\n",
        "      joints_sequence = self.dropout(joints_sequence)\n",
        "      joints_sequence = joints_sequence.permute(0, 1, 3, 2)\n",
        "\n",
        "    joints_sequence = joints_sequence.reshape(joints_sequence.shape[0], joints_sequence.shape[1], -1) #in order to pass to lstm a 3D sequence\n",
        "\n",
        "    #with \"pack_padded_sequence\", we create a PackedSequence object that masks the padding frames, but makes the joints_sequence still an accettable input for the LSTM\n",
        "    packed_joints_sequence = torch.nn.utils.rnn.pack_padded_sequence(joints_sequence, lengths=seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    self.lstm.flatten_parameters()\n",
        "\n",
        "    packed_output,(h_n, _) = self.lstm(packed_joints_sequence)\n",
        "    \n",
        "    output = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)[0] #retrieve the actual output data\n",
        "\n",
        "    if self.bidirectional==True:\n",
        "      h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)  # shape=(batch_size, h_s*2), concat final hidden states of Bidirectional-LSTM\n",
        "    else:\n",
        "      h_final = h_n[-1] #shape=(batch_size, h_s)\n",
        "    \n",
        "    if self.apply_weights == True:\n",
        "      attention_input = output.reshape(-1,self.hidden_size) #shape=(batch_size*num_frames, hidden_size)\n",
        "\n",
        "      query = self.query(h_final).unsqueeze(1)  #shape=(batch_size, 1, hidden_size)\n",
        "      key = self.key(attention_input) #shape=(batch_size*num_frames, hidden_size)\n",
        "      value = self.value(attention_input) #shape=(batch_size*num_frames, hidden_size)\n",
        "\n",
        "      key = key.reshape(output.shape[0], output.shape[1], -1) #(batch_size, num_frames, hidden_size)\n",
        "      value = value.reshape(output.shape[0], output.shape[1], -1) #(batch_size, num_frames, hidden_size)\n",
        "\n",
        "      energy =  torch.bmm(query,key.transpose(1,2)) #shape=(batch_size, 1, hidden_size) * (batch_size, hidden_size, num_frames) = (batch_size, 1, num_frames)\n",
        "      attention = self.softmax(energy) \n",
        "\n",
        "      weights = torch.bmm(attention,value) #shape=(batch_size, 1, hidden_size)\n",
        "\n",
        "      weights = weights.squeeze(1) #shape=(batch_size, hidden_size)\n",
        "\n",
        "      h_final = h_final + weights #add the weights to the output of LSTM, that will be passed to the fully connected layer\n",
        "    \n",
        "    if self.apply_weights==True:\n",
        "      fc_out= self.classifier(h_final)\n",
        "      return fc_out\n",
        "    else:\n",
        "      output = output.reshape(-1, self.hidden_size)\n",
        "      fc_out= self.classifier(output)\n",
        "      #Reshape dell'output per tornare alla forma (batch_size, num_frames, num_classes)\n",
        "      fc_out = fc_out.reshape(sequence.shape[0], -1, self.num_classes)\n",
        "      #To have only one prediction distribution for each video, we compute the mean over the first dimension of fc_out\n",
        "      fc_out_avg = fc_out.mean(axis=1)#shape=(batch_size, num_classes)\n",
        "      return fc_out_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nkyfL9acXWtk"
      },
      "outputs": [],
      "source": [
        "class TennisShotsTrainer:\n",
        "      def __init__(self, train_dataloader, valid_dataloader, model, optimizer, loss, num_classes, \n",
        "                   metric, metric_weighted, device, save_folder, num_features, batch_size):\n",
        "        \n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = valid_dataloader\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = loss\n",
        "        self.num_classes = num_classes\n",
        "        self.metric = metric\n",
        "        self.metric_weighted = metric_weighted\n",
        "        self.device = device\n",
        "        self.save_folder = save_folder\n",
        "        \n",
        "        self.history = {\n",
        "                        \"NUM_EPOCHS\" : NUM_EPOCHS,\n",
        "                        \"NUM_HIDDEN\" : NUM_HIDDEN,\n",
        "                        \"NUM_FEATURES\" : num_features,\n",
        "                        \"NUM_LAYERS\" : NUM_LAYERS,\n",
        "                        \"BATCH_SIZE\" : batch_size,\n",
        "                        \"DROPOUT\" : DROPOUT,\n",
        "                        \"LEARNING_RATE\" : LEARNING_RATE,\n",
        "                        \"APPLY_WEIGHTS\" : APPLY_WEIGHTS,\n",
        "                        \"BIDIRECTIONAL\" : BIDIRECTIONAL,\n",
        "                        \"DROPOUT_2D\" : DROPOUT_2D,\n",
        "                        \"train_acc\":[], \n",
        "                        \"train_loss\":[], \n",
        "                        \"train_f1\":[], \n",
        "                        \"train_f1_weighted\":[], \n",
        "                        \"valid_acc\":[], \n",
        "                        \"valid_loss\":[], \n",
        "                        \"valid_f1\":[], \n",
        "                        \"valid_f1_weighted\":[] \n",
        "                        }\n",
        "\n",
        "      def train_and_validate(self):\n",
        "          \n",
        "          num_epochs = NUM_EPOCHS\n",
        "\n",
        "          t = datetime.datetime.now()\n",
        "          minute = str(t.minute)\n",
        "          if t.minute < 10:\n",
        "            minute = '0'+minute\n",
        "          daytime = str(t.year) + ':' + str(t.month) + ':' + str(t.day) + '-' + str(t.hour) + ':' + minute\n",
        "\n",
        "          save_path = self.save_folder + '/' + daytime\n",
        "\n",
        "          if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "          best_avg_valid_f1_weighted = -100\n",
        "\n",
        "          for epoch in range(1, num_epochs+1):\n",
        "\n",
        "            if epoch < 10:\n",
        "              print(f'\\nEpoch 0{epoch}:')\n",
        "            if epoch >= 10:\n",
        "              print(f'\\nEpoch {epoch}:')\n",
        "\n",
        "            # TRAINING STEP\n",
        "\n",
        "            #parameters useful to compute accuracy\n",
        "            train_correct = 0\n",
        "            train_total = len(self.train_dataloader.dataset) # number of videos in the training set\n",
        "            train_total_batches = len(self.train_dataloader) # number of batches in the training set (self.train_dataloader.dataset / BATCH_SIZE) \n",
        "            #compute and save loss and f1-score\n",
        "            train_epoch_loss = 0\n",
        "            train_epoch_f1_score = torch.zeros(10).to(self.device)\n",
        "            train_epoch_f1_score_weighted = 0\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "            for x_train , y_train in self.train_dataloader:\n",
        "              #Forward Pass\n",
        "              x_train , y_train = x_train.to(self.device), y_train.to(self.device)\n",
        "              self.optimizer.zero_grad()\n",
        "              pred = self.model(x_train)\n",
        "              #to compute loss, we have to convert true labels to \"one-hot\" format\n",
        "              one_hot = (torch.eye(self.num_classes)).to(self.device)\n",
        "              y_train_one_hot = one_hot[y_train]\n",
        "              loss = self.loss(pred,y_train_one_hot)\n",
        "              #BackPropagation\n",
        "              loss.backward()\n",
        "              #Optimization\n",
        "              self.optimizer.step()\n",
        "              #Update total train epoch loss\n",
        "              train_epoch_loss += loss\n",
        "              #Compute Accuracy\n",
        "              predicted_maxvalues= torch.argmax(pred, dim=1).to(self.device) #unidimensional tensor with size=BATCH_SIZE \n",
        "              train_correct_vector = (predicted_maxvalues==y_train)\n",
        "              train_correct += train_correct_vector.sum().item()\n",
        "              #Compute f1-Score\n",
        "              train_epoch_f1_score += self.metric(predicted_maxvalues, y_train)\n",
        "              train_epoch_f1_score_weighted += self.metric_weighted(predicted_maxvalues, y_train).item()\n",
        "            #compute epoch average loss, accuracy, f1\n",
        "            avg_train_loss = train_epoch_loss / train_total_batches #we compute the avg loss for each batch, because otherwise we would depend on the size of the dataset\n",
        "            avg_train_loss_np = avg_train_loss.cpu().detach().numpy() #to put it into self.history\n",
        "            avg_train_acc = train_correct / train_total\n",
        "            avg_train_f1 = train_epoch_f1_score / train_total_batches\n",
        "            avg_train_f1_np = avg_train_f1.cpu().numpy() #to put it into self.history\n",
        "            avg_train_f1_weighted = train_epoch_f1_score_weighted / train_total_batches\n",
        "            #print them\n",
        "            print('{} Loss: {:.4f} Accuracy: {:.4f} F1_Score: {:.4f}'.format('train', avg_train_loss, avg_train_acc, avg_train_f1_weighted))\n",
        "\n",
        "\n",
        "            # VALIDATION STEP\n",
        "\n",
        "            #parameters useful to compute accuracy\n",
        "            valid_correct = 0\n",
        "            valid_total = len(self.val_dataloader.dataset) # number of videos in the validation set\n",
        "            valid_total_batches = len(self.val_dataloader) # number of batches in the validation set (self.val_dataloader.dataset / BATCH_SIZE) \n",
        "            #compute and save loss and f1-score\n",
        "            valid_epoch_loss = 0\n",
        "            valid_epoch_f1_score = torch.zeros(10).to(self.device)\n",
        "            valid_epoch_f1_score_weighted = 0\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "              for x_valid , y_valid in self.val_dataloader:\n",
        "                #Forward Pass\n",
        "                x_valid , y_valid = x_valid.to(self.device), y_valid.to(self.device)\n",
        "                pred = self.model(x_valid)\n",
        "                #to compute loss, we have to convert true labels to \"one-hot\" format\n",
        "                one_hot = (torch.eye(self.num_classes)).to(self.device)\n",
        "                y_valid_one_hot = one_hot[y_valid]\n",
        "                loss = self.loss(pred,y_valid_one_hot)\n",
        "                valid_epoch_loss += loss\n",
        "                #Compute Accuracy\n",
        "                predicted_maxvalues = torch.argmax(pred, dim=1).to(self.device) #unidimensional tensor with size=BATCH_SIZE \n",
        "                valid_correct += (predicted_maxvalues==y_valid).sum().item()\n",
        "                #Compute f1-Score\n",
        "                valid_epoch_f1_score += self.metric(predicted_maxvalues, y_valid)\n",
        "                valid_epoch_f1_score_weighted += self.metric_weighted(predicted_maxvalues, y_valid).item()\n",
        "              \n",
        "            #epoch average loss, accuracy, f1\n",
        "            avg_valid_loss = valid_epoch_loss / valid_total_batches\n",
        "            avg_valid_loss_np = avg_valid_loss.cpu().detach().numpy()\n",
        "            avg_valid_acc = valid_correct / valid_total\n",
        "            avg_valid_f1 = (valid_epoch_f1_score / valid_total_batches)\n",
        "            avg_valid_f1_np = avg_valid_f1.cpu().numpy()\n",
        "            avg_valid_f1_weighted = valid_epoch_f1_score_weighted / valid_total_batches\n",
        "            #print them\n",
        "            print('{} Loss: {:.4f} Accuracy: {:.4f} F1_Score: {:.4f}'.format('validation', avg_valid_loss, avg_valid_acc, avg_valid_f1_weighted))\n",
        "\n",
        "\n",
        "            # SAVE STEP\n",
        "            self.history[\"train_acc\"].append(avg_train_acc)\n",
        "            self.history[\"train_loss\"].append(avg_train_loss_np)\n",
        "            self.history[\"train_f1\"].append(avg_train_f1_np)\n",
        "            self.history[\"train_f1_weighted\"].append(avg_train_f1_weighted)\n",
        "            self.history[\"valid_acc\"].append(avg_valid_acc)\n",
        "            self.history[\"valid_loss\"].append(avg_valid_loss_np)\n",
        "            self.history[\"valid_f1\"].append(avg_valid_f1_np)\n",
        "            self.history[\"valid_f1_weighted\"].append(avg_valid_f1_weighted)\n",
        "\n",
        "            #for each epoch, verify that the model obtained by training+validating is the best yet => saves only the best\n",
        "            best_model_path = save_path + '/model.pth'\n",
        "            if avg_valid_f1_weighted > best_avg_valid_f1_weighted:\n",
        "              torch.save(self.model.state_dict(), best_model_path)\n",
        "              best_avg_valid_f1_weighted = avg_valid_f1_weighted\n",
        "\n",
        "          #for each training, save history and return it\n",
        "          history_df = pd.DataFrame(self.history)\n",
        "          history_path = save_path + '/history.csv'\n",
        "          history_df.to_csv(history_path, index=False)\n",
        "          return self.history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mqv0CzSy-HW"
      },
      "outputs": [],
      "source": [
        "#try to execute training on GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "#create folder to save best model and history\n",
        "save_folder = '/content/drive/MyDrive/Vision_Project_Utils/Training'\n",
        "try:\n",
        "  os.makedirs(save_folder)\n",
        "except OSError as e1:\n",
        "  print(\"Creation of the directory %s failed\" % save_folder)\n",
        "  print(\"Error:\", e1)\n",
        "  print(traceback.format_exc())\n",
        "\n",
        "#initialize parameters needed by the trainer\n",
        "model = TennisShotsClassificationLSTM(NUM_HIDDEN,NUM_FEATURES,NUM_CLASSES,NUM_LAYERS,BATCH_SIZE,DROPOUT, APPLY_WEIGHTS, BIDIRECTIONAL).to(device)\n",
        "loss = nn.CrossEntropyLoss() \n",
        "#optimizer = optim.Adam(model.parameters(), LEARNING_RATE) \n",
        "#optimizer = optim.RMSprop(model.parameters(), LEARNING_RATE, momentum=0.9, weight_decay=0.001) \n",
        "optimizer = optim.Adagrad(model.parameters(), LEARNING_RATE)\n",
        "metric = MulticlassF1Score(num_classes=NUM_CLASSES, average=None).to(device)\n",
        "metric_weighted = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro', zero_division='ignore').to(device)\n",
        "\n",
        "#initialize a \"TennisShotsTrainer\" object\n",
        "MrTrain = TennisShotsTrainer(train_dataloader, val_dataloader, model, optimizer,loss, NUM_CLASSES, metric, metric_weighted, device, save_folder, NUM_FEATURES, BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform training and validation => return history\n",
        "history = MrTrain.train_and_validate()"
      ],
      "metadata": {
        "id": "ffn8pK5vWnr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t303q77evcpd"
      },
      "outputs": [],
      "source": [
        "#Plot Loss, Accuracy and F1-Score progresses over the epochs\n",
        "epochs = range(0, NUM_EPOCHS)\n",
        "fig, axs = plt.subplots(1, 3, figsize=(35, 8))\n",
        "axs[0].plot(epochs, history[\"train_loss\"], label='Training Loss')\n",
        "axs[0].plot(epochs, history[\"valid_loss\"], label='Validation Loss')\n",
        "axs[1].plot(epochs, history[\"train_acc\"], label='Training Accuracy')\n",
        "axs[1].plot(epochs, history[\"valid_acc\"], label='Validation Accuracy')\n",
        "axs[2].plot(epochs, history[\"train_f1_weighted\"], label='Training F1-Score')\n",
        "axs[2].plot(epochs, history[\"valid_f1_weighted\"], label='Validation F1-Score')\n",
        "\n",
        "fig.suptitle('Comparison between Training and Validation Phases')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('F1-Score')\n",
        "\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "axs[2].legend()\n",
        "\n",
        "axs[0].set_ylim(0, 5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJxh2kfuZSfO"
      },
      "source": [
        "# **TESTING PHASE**\n",
        "Define the test method, execute it with test_dataloader and the Trainer object.\n",
        "\n",
        "The returned lists are used to compute and print the Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oGualKyyZRuv"
      },
      "outputs": [],
      "source": [
        "def test(test_dl, model):\n",
        "  \n",
        "  #TEST STEP\n",
        "\n",
        "  #parameters useful to compute accuracy\n",
        "  test_correct = 0\n",
        "  test_total = len(test_dl.dataset) # number of videos in the test set\n",
        "  test_total_batches = len(test_dl) # number of batches in the test set (test_dl.dataset / BATCH_SIZE) \n",
        "  #compute and save loss and f1-score\n",
        "  test_loss = 0\n",
        "  test_f1 = torch.zeros(10).to(device)\n",
        "  test_f1_weighted = 0\n",
        "\n",
        "  #to compute the Confusion Matrix, we have to return the lists containing the true and predicted labels  \n",
        "  y_true_total = []\n",
        "  y_pred_total = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x_test , y_test in test_dl:\n",
        "      y_true_total.extend(y_test.tolist())\n",
        "      #Forward Pass\n",
        "      x_test , y_test = x_test.to(device), y_test.to(device)\n",
        "      pred = model(x_test)\n",
        "      #to compute loss, we have to convert true labels to \"one-hot\" format\n",
        "      one_hot = (torch.eye(NUM_CLASSES)).to(device)\n",
        "      y_test_one_hot = one_hot[y_test]\n",
        "      new_loss = loss(pred,y_test_one_hot)\n",
        "      test_loss += new_loss\n",
        "      #Compute Accuracy\n",
        "      predicted_maxvalues = torch.argmax(pred, dim=1) #unidimensional tensor with size=BATCH_SIZE \n",
        "      y_pred = predicted_maxvalues.cpu().tolist() \n",
        "      y_pred_total.extend(y_pred)\n",
        "      predicted_maxvalues = predicted_maxvalues.to(device)\n",
        "      test_correct += (predicted_maxvalues==y_test).sum().item()\n",
        "      #Compute f1-Score \n",
        "      test_f1 += metric(predicted_maxvalues, y_test)\n",
        "      test_f1_weighted += metric_weighted(predicted_maxvalues, y_test).item()\n",
        "           \n",
        "  #average loss, accuracy, f1\n",
        "  avg_test_loss = test_loss / test_total_batches\n",
        "  avg_test_acc = test_correct / test_total\n",
        "  avg_test_f1 = (test_f1 / test_total_batches)\n",
        "  avg_test_f1_weighted = test_f1_weighted / test_total_batches\n",
        "  #print them\n",
        "  print('{} Loss: {:.4f} Accuracy: {:.4f} F1_Score: {:.4f}'.format('test', avg_test_loss, avg_test_acc, avg_test_f1_weighted))\n",
        "\n",
        "  return y_true_total, y_pred_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "LbMQKqwEKqyy",
        "outputId": "195d6fcf-5f8e-4219-f312-fc65d61eb328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Loss: 0.7056 Accuracy: 0.7623 F1_Score: 0.6726\n",
            "+------------+-----------+--------+\n",
            "|    Shot    | Precision | Recall |\n",
            "+------------+-----------+--------+\n",
            "|  backhand  |    0.94   |  0.91  |\n",
            "| backhand2h |    0.9    |  1.0   |\n",
            "|  serflat   |    0.79   |  0.58  |\n",
            "|  foreflat  |    0.93   |  0.79  |\n",
            "|  foreopen  |    0.91   |  0.88  |\n",
            "|   fslice   |    0.68   |  0.76  |\n",
            "|  fvolley   |    0.71   |  0.73  |\n",
            "|  serkick   |    0.6    |  0.85  |\n",
            "|  serslice  |    0.57   |  0.7   |\n",
            "|   smash    |    0.8    |  0.48  |\n",
            "+------------+-----------+--------+\n",
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#best_model = TennisShotsClassificationLSTM(NUM_HIDDEN,NUM_FEATURES,NUM_CLASSES,NUM_LAYERS,BATCH_SIZE,DROPOUT, APPLY_WEIGHTS, BIDIRECTIONAL).to(device)\n",
        "\n",
        "#best_model.load_state_dict(torch.load('/content/drive/MyDrive/Vision_Project_Utils/model.pth'))\n",
        "\n",
        "#y_true, y_pred = test(test_dataloader, best_model)\n",
        "\n",
        "y_true, y_pred = test(test_dataloader, model) #if you use \"best_model\", comment this!\n",
        "\n",
        "#Print Precision-Recall for each class\n",
        "precision = precision_score(y_true, y_pred, average=None)\n",
        "precision_rounded = np.round(precision, 2)\n",
        "recall = recall_score(y_true, y_pred, average=None)\n",
        "recall_rounded = np.round(recall, 2)\n",
        "\n",
        "class_names = ['backhand',\n",
        "    'backhand2h',\n",
        "    'serflat',\n",
        "    'foreflat',\n",
        "    'foreopen',\n",
        "    'fslice',\n",
        "    'fvolley',\n",
        "    'serkick',\n",
        "    'serslice',\n",
        "    'smash']\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = ['Shot', 'Precision', 'Recall']\n",
        "\n",
        "df = pd.DataFrame({'Precision': precision_rounded, 'Recall': recall_rounded}, index=class_names)\n",
        "for label in df.index:\n",
        "  table.add_row([label, df.loc[label][\"Precision\"], df.loc[label][\"Recall\"]])\n",
        "\n",
        "print(table.get_string())\n",
        "\n",
        "# Build the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "#compute percentages\n",
        "cm_percentages = cm / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "fmt = lambda x: '{:.1f}%'.format(x)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "sns.heatmap(cm_percentages, annot=True, fmt=\".1f\", cmap='Blues', annot_kws={'size':10}, xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L23NlcPhHVke",
        "Ub8ynPlaxdIa",
        "ElaxDNXe5NRs",
        "djLkRQM_VZrl",
        "WrC7Ce2byMRW",
        "yJxh2kfuZSfO"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}